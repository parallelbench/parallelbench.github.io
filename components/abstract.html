<section id="abstract" class="bg-white py-12 sm:py-16 lg:py-24">
  <div class="mx-auto max-w-6xl px-6 lg:px-12">
    <div class="text-center">
      <span
        class="inline-block rounded-full bg-primary/10 px-4 py-1.5 text-sm font-semibold text-primary"
      >
        Full Paper
      </span>
      <h2 class="mt-6 text-4xl font-bold text-slateInk">Abstract</h2>
    </div>

    <!-- Mobile: collapsed view without card -->
    <div class="mt-12 lg:hidden">
      <div class="space-y-4 text-base leading-relaxed text-slate-600">
        <!-- Read more button (visible when collapsed) -->
        <div id="readMoreContainer" class="text-center">
          <p id="abstractPreview" class="mb-4">
            While most autoregressive LLMs are constrained to one-by-one
            decoding, diffusion LLMs (dLLMs) have attracted growing interest for
            their potential to dramatically accelerate inference through
            parallel decoding. Despite this promise, the conditional
            independence assumption in dLLMs causes parallel decoding to ignore
            token dependencies...
          </p>
          <button
            id="readMoreBtn"
            onclick="toggleAbstract()"
            class="inline-flex items-center gap-2 rounded-lg bg-primary/10 px-4 py-2 text-sm font-semibold text-primary transition hover:bg-primary/20 focus-visible:outline focus-visible:outline-2 focus-visible:outline-primary"
          >
            <span>Read more</span>
            <svg
              class="h-4 w-4"
              fill="none"
              stroke="currentColor"
              viewBox="0 0 24 24"
            >
              <path
                stroke-linecap="round"
                stroke-linejoin="round"
                stroke-width="2"
                d="M19 9l-7 7-7-7"
              />
            </svg>
          </button>
        </div>

        <!-- Full content with Read less button (visible when expanded) -->
        <div id="readLessContainer" class="hidden">
          <button
            id="readLessBtn"
            onclick="toggleAbstract()"
            class="mb-4 mx-auto block text-sm font-medium text-slate-500 hover:text-primary transition underline underline-offset-4 decoration-slate-300 hover:decoration-primary"
          >
            Read less â†‘
          </button>
          <p id="abstractFull">
            While most autoregressive LLMs are constrained to one-by-one
            decoding, diffusion LLMs (dLLMs) have attracted growing interest for
            their potential to dramatically accelerate inference through
            parallel decoding. Despite this promise, the conditional
            independence assumption in dLLMs causes parallel decoding to ignore
            token dependencies, inevitably degrading generation quality when
            these dependencies are strong. However, existing works largely
            overlook these inherent challenges, and evaluations on standard
            benchmarks (e.g., math and coding) are not sufficient to capture the
            quality degradation caused by parallel decoding. To address this
            gap, we first provide an information-theoretic analysis of parallel
            decoding. We then conduct case studies on analytically tractable
            synthetic list operations from both data distribution and decoding
            strategy perspectives, offering quantitative insights that highlight
            the fundamental limitations of parallel decoding. Building on these
            insights, we propose ParallelBench, the first benchmark specifically
            designed for dLLMs, featuring realistic tasks that are trivial for
            humans and autoregressive LLMs yet exceptionally challenging for
            dLLMs under parallel decoding. Using ParallelBench, we
            systematically analyze both dLLMs and autoregressive LLMs, revealing
            that: (i) dLLMs under parallel decoding can suffer dramatic quality
            degradation in real-world scenarios, and (ii) current parallel
            decoding strategies struggle to adapt their degree of parallelism
            based on task difficulty, thus failing to achieve meaningful speedup
            without compromising quality. Our findings underscore the pressing
            need for innovative decoding methods that can overcome the current
            speed-quality trade-off. We are releasing our benchmark to help
            accelerate the development of truly efficient dLLMs.
          </p>
        </div>
      </div>
    </div>

    <!-- Desktop: full view in card -->
    <div
      class="mt-12 hidden lg:block rounded-3xl border-2 border-slate-200 bg-white p-8 lg:p-12 shadow-xl"
    >
      <div class="space-y-6 text-justify text-base leading-8 text-slate-600">
        <p>
          While most autoregressive LLMs are constrained to one-by-one decoding,
          diffusion LLMs (dLLMs) have attracted growing interest for their
          potential to dramatically accelerate inference through parallel
          decoding. Despite this promise, the conditional independence
          assumption in dLLMs causes parallel decoding to ignore token
          dependencies, inevitably degrading generation quality when these
          dependencies are strong. However, existing works largely overlook
          these inherent challenges, and evaluations on standard benchmarks
          (e.g., math and coding) are not sufficient to capture the quality
          degradation caused by parallel decoding. To address this gap, we first
          provide an information-theoretic analysis of parallel decoding. We
          then conduct case studies on analytically tractable synthetic list
          operations from both data distribution and decoding strategy
          perspectives, offering quantitative insights that highlight the
          fundamental limitations of parallel decoding. Building on these
          insights, we propose ParallelBench, the first benchmark specifically
          designed for dLLMs, featuring realistic tasks that are trivial for
          humans and autoregressive LLMs yet exceptionally challenging for dLLMs
          under parallel decoding. Using ParallelBench, we systematically
          analyze both dLLMs and autoregressive LLMs, revealing that: (i) dLLMs
          under parallel decoding can suffer dramatic quality degradation in
          real-world scenarios, and (ii) current parallel decoding strategies
          struggle to adapt their degree of parallelism based on task
          difficulty, thus failing to achieve meaningful speedup without
          compromising quality. Our findings underscore the pressing need for
          innovative decoding methods that can overcome the current
          speed-quality trade-off. We are releasing our benchmark to help
          accelerate the development of truly efficient dLLMs.
        </p>
      </div>
    </div>

    <div class="mt-12 flex justify-center">
      <a
        href="assets/pdf/ParallelBench.pdf"
        class="group inline-flex items-center gap-3 rounded-2xl border-2 border-primary bg-gradient-to-r from-primary to-primary/90 px-10 py-5 text-lg font-bold text-white shadow-xl transition-all hover:shadow-2xl hover:scale-105"
        target="_blank"
        rel="noopener noreferrer"
      >
        <svg
          class="h-6 w-6"
          fill="none"
          stroke="currentColor"
          viewBox="0 0 24 24"
        >
          <path
            stroke-linecap="round"
            stroke-linejoin="round"
            stroke-width="2"
            d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"
          />
        </svg>
        <span>Read the Full Paper</span>
        <svg
          class="h-5 w-5 transition-transform group-hover:translate-x-1"
          fill="none"
          stroke="currentColor"
          viewBox="0 0 24 24"
        >
          <path
            stroke-linecap="round"
            stroke-linejoin="round"
            stroke-width="2"
            d="M13 7l5 5m0 0l-5 5m5-5H6"
          />
        </svg>
      </a>
    </div>
  </div>
</section>
